
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Chapter 17 – Autoencoders and GANs** &#8212; Data Science Foundation in Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://jads-nl.github.io/data-science-foundation-curriculum/handson-ml2/original/17_autoencoders_and_gans.html" />
    <link rel="shortcut icon" href="../../_static/favicon-32x32.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Chapter 18 – Reinforcement Learning" href="18_reinforcement_learning.html" />
    <link rel="prev" title="Chapter 16 – Natural Language Processing with RNNs and Attention**" href="16_nlp_with_rnns_and_attention.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/jads-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science Foundation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    Why this JupyterBook?
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Statistical Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../islr/introduction.html">
   Introduction to ISLRv2
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../islr/labs.html">
   Labs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_02.3_introduction.html">
     Lab 2.3: Introduction to Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_03.6_linear_regression.html">
     Lab 3.6: Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_04.7_classification_methods.html">
     Lab 4.7: Classification Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_05.3_cross_validation_and_the_bootstrap.html">
     Lab 5.3: Cross-Validation and the Bootstrap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_06.5.1_subset_selection_methods.html">
     Lab 6.5.1: Subset Selection Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_06.5.2_ridge_regression_and_the_lasso.html">
     Lab 6.5.2: Ridge Regression and the Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_06.5.3_pcr_and_pls_regression.html">
     Lab 6.5.3: PCR and PLS Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_07.8_non_linear_modelling.html">
     Lab 7.8: Non-linear Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_08.3_decision_trees.html">
     Lab 8.3: Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_09.6_support_vector_machines.html">
     Lab 9.6: Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_10.9.2_multilayer_network_on_mnist_digit_data.html">
     Lab 10.9.2: A Multilayer Network on the MNIST Digit Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_10.9.3_convolutional_neural_networks.html">
     Lab 10.9.3: Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_10.9.5_imdb_document_classification.html">
     Lab 10.9.5: IMDb Document Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_10.9.6_recurrent_neural_networks.html">
     Lab 10.9.6: Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../islr/labs/lab_12.5_unsupervised_learning.html">
     Lab 12.5: Unsupervised Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../islr/exercises.html">
   Exercises
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter2/index.html">
     Chapter 2
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise1.html">
       Exercise 2.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise2.html">
       Exercise 2.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise3.html">
       Exercise 2.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise4.html">
       Exercise 2.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise5.html">
       Exercise 2.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise6.html">
       Exercise 2.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise7.html">
       Exercise 2.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise8.html">
       Exercise 2.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise9.html">
       Exercise 2.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter2/exercise10.html">
       Exercise 2.10
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter3/index.html">
     Chapter 3
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise1.html">
       Exercise 3.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise2.html">
       Exercise 3.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise3.html">
       Exercise 3.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise4.html">
       Exercise 3.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise5.html">
       Exercise 3.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise6.html">
       Exercise 3.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise7.html">
       Exercise 3.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise8.html">
       Exercise 3.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise9.html">
       Exercise 3.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise10.html">
       Exercise 3.10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise11.html">
       Exercise 3.11
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise12.html">
       Exercise 3.12
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise13.html">
       Exercise 3.13
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise14.html">
       Exercise 3.14
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter3/exercise15.html">
       Exercise 3.15
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter4/index.html">
     Chapter 4
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise1.html">
       Exercise 4.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise2.html">
       Exercise 4.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise3.html">
       Exercise 4.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise4.html">
       Exercise 4.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise5.html">
       Exercise 4.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise6.html">
       Exercise 4.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise7.html">
       Exercise 4.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise8.html">
       Exercise 4.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise9.html">
       Exercise 4.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise10.html">
       Exercise 10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise11.html">
       Exercise 11
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise12.html">
       Exercise 12
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise13.html">
       Exercise 4.13
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise14.html">
       Exercise 4.11
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise15.html">
       Exercise 4.12
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter4/exercise16.html">
       Exercise 4.16
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter5/index.html">
     Chapter 5
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise1.html">
       Exercise 5.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise2.html">
       Exercise 5.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise3.html">
       Problem 5.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise4.html">
       Exercise 5.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise5.html">
       Exercise 5.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise6.html">
       Exercise 5.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise7.html">
       Exercise 5.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise8.html">
       Exercise 5.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter5/exercise9.html">
       Exercise 5.9
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter6/index.html">
     Chapter 6
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise1.html">
       Exercise 6.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise2.html">
       Exercise 6.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise3.html">
       Exercise 6.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise4.html">
       Exercise 6.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise5.html">
       Exercise 6.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise6.html">
       Exercise 6.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise7.html">
       Exercise 6.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise8.html">
       Exercise 6.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise9.html">
       Exercise 6.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise10.html">
       Exercise 6.10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter6/exercise11.html">
       Exercise 6.11
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter7/index.html">
     Chapter 7
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise1.html">
       Exercise 7.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise2.html">
       Exercise 7.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise3.html">
       Exercise 7.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise4.html">
       Exercise 7.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise5.html">
       Exercise 7.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise6.html">
       Exercise 7.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise7.html">
       Exercise 7.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise8.html">
       Exercise 7.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise9.html">
       Exercise 7.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise10.html">
       Exercise 7.10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise11.html">
       Exercise 7.11
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter7/exercise12.html">
       Exercise 7.12
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter8/index.html">
     Chapter 8
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise1.html">
       Exercise 8.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise2.html">
       Exercise 8.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise3.html">
       Exercise 8.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise4.html">
       Exercise 8.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise5.html">
       Exercise 8.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise6.html">
       Exercise 8.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise7.html">
       Exercise 8.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise8.html">
       Exercise 8.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise9.html">
       Exercise 8.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise10.html">
       Exercise 8.10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise11.html">
       Exercise 8.11
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter8/exercise12.html">
       Exercise 8.12
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter9/index.html">
     Chapter 9
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise1.html">
       Exercise 9.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise2.html">
       Exercise 9.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise3.html">
       Exercise 9.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise4.html">
       Exercise 9.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise5.html">
       Exercise 9.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise6.html">
       Exercise 9.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise7.html">
       Exercise 9.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter9/exercise8.html">
       Exercise 9.8
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../islr/exercises/chapter12/index.html">
     Chapter 12
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise1.html">
       Exercise 12.1
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise2.html">
       Exercise 12.2
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise3.html">
       Exercise 12.3
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise4.html">
       Exercise 12.4
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise5.html">
       Exercise 12.5
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise6.html">
       Exercise 12.6
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise7.html">
       Exercise 12.7
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise8.html">
       Exercise 10.8
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise9.html">
       Exercise 12.9
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise10.html">
       Exercise 12.10
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../islr/exercises/chapter12/exercise11.html">
       Exercise 12.11
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hands-on Machine Learning in Python
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../introduction.html">
   Hands-on Machine Learning with Aurélien Géron
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01_the_machine_learning_landscape.html">
     Chapter 1 – The Machine Learning landscape
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_end_to_end_machine_learning_project.html">
     Chapter 2 – End-to-end Machine Learning project
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_classification.html">
     Chapter 3 – Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_training_linear_models.html">
     Chapter 4 – Training Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="05_support_vector_machines.html">
     Chapter 5 – Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06_decision_trees.html">
     Chapter 6 – Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07_ensemble_learning_and_random_forests.html">
     Chapter 7 – Ensemble Learning and Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08_dimensionality_reduction.html">
     Chapter 8 – Dimensionality Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_unsupervised_learning.html">
     Chapter 9 – Unsupervised Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10_neural_nets_with_keras.html">
     Chapter 10 – Introduction to Artificial Neural Networks with Keras
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="11_training_deep_neural_networks.html">
     Chapter 11 – Training Deep Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12_custom_models_and_training_with_tensorflow.html">
     Chapter 12 – Custom Models and Training with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13_loading_and_preprocessing_data.html">
     Chapter 13 – Loading and Preprocessing Data with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14_deep_computer_vision_with_cnns.html">
     Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="15_processing_sequences_using_rnns_and_cnns.html">
     Chapter 15 – Processing Sequences Using RNNs and CNNs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="16_nlp_with_rnns_and_attention.html">
     Chapter 16 – Natural Language Processing with RNNs and Attention**
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Chapter 17 – Autoencoders and GANs**
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="18_reinforcement_learning.html">
     Chapter 18 – Reinforcement Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="19_training_and_deploying_at_scale.html">
     Chapter 19 – Training and Deploying TensorFlow Models at Scale
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../extra.html">
   Miscellaneous tips &amp; tricks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="math_linear_algebra.html">
     Math primer - Linear Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="math_differential_calculus.html">
     Math primer - Differential Calculus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="extra_autodiff.html">
     Implementation of autodiff techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="extra_gradient_descent_comparison.html">
     Comparison of Batch, Mini-Batch and Stochastic Gradient Descent
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Data visualization with Altair
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../visualization/introduction.html">
   Data visualization with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../visualization/introduction-to-interactive-data-visualization.html">
   Introduction to data visualization with Python
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../visualization/altair_get_started.html">
   Getting Started with Altair
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_introduction.html">
     Introduction to Altair
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_marks_encoding.html">
     Data Types, Graphical Marks, and Visual Encoding Channels
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_data_transformation.html">
     Data Transformation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_scales_axes_legends.html">
     Scales, Axes, and Legends
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_view_composition.html">
     Multi-View Composition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_interaction.html">
     Interaction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_cartographic.html">
     Cartographic Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../visualization/altair_debugging.html">
     Altair Debugging Guide
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Interpretable Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../iml/introduction.html">
   Introduction to IML
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../iml/plot_partial_dependence.html">
     Partial Dependence and Individual Conditional Expectation Plots
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../iml/plot_permutation_importance.html">
     Permutation Importance vs Random Forest Feature Importance (MDI)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../iml/plot_permutation_importance_multicollinear.html">
     Permutation Importance with Multicollinear or Correlated Features
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../iml/interpreting-machine-learning-models.html">
     IML with Pima Indians and the Titanic datasets
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Principles of time-series forecasting
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../fpp/introduction.html">
   Introduction to FPP3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../fpp/examples-time-series-forecasting-with-python.html">
   Examples of time-series forecasting with Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural language processing with spaCy
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/introduction.html">
   Introduction to NLP with spaCy
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../nlp/dutch-restaurant-reviews.html">
   Dutch restaurant reviews
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/nlp-dutch-restaurant-reviews.html">
     Analyzing Dutch restaurant reviews
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/prepare-restoreviews-dataset.html">
     Fetch and prepare reviews
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/nlp-pipeline-example.html">
     Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/language-detection-with-character-quadgrams.html">
     Extra: language detection with character quadgrams
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../nlp/nlp-model-sizes.html">
     Extra: plot of transformer model sizes
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jads-nl/data-science-foundation-in-python/main?urlpath=tree/data_science_foundation_in_python/handson-ml2/original/17_autoencoders_and_gans.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/jads-nl/data-science-foundation-in-python/blob/main/data_science_foundation_in_python/handson-ml2/original/17_autoencoders_and_gans.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jads-nl/data-science-foundation-in-python"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jads-nl/data-science-foundation-in-python/issues/new?title=Issue%20on%20page%20%2Fhandson-ml2/original/17_autoencoders_and_gans.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/handson-ml2/original/17_autoencoders_and_gans.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Chapter 17 – Autoencoders and GANs**
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-with-a-linear-autoencoder">
   PCA with a linear Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-autoencoders">
   Stacked Autoencoders
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-all-layers-at-once">
     Train all layers at once
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualizing-fashion-mnist">
   Visualizing Fashion MNIST
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tying-weights">
     Tying weights
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-one-autoencoder-at-a-time">
     Training one Autoencoder at a Time
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-convolutional-layers-instead-of-dense-layers">
     Using Convolutional Layers Instead of Dense Layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recurrent-autoencoders">
   Recurrent Autoencoders
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-denoising-autoencoder">
   Stacked denoising Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-autoencoder">
   Sparse Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-autoencoder">
   Variational Autoencoder
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-fashion-images">
     Generate Fashion Images
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-adversarial-networks">
   Generative Adversarial Networks
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-convolutional-gan">
   Deep Convolutional GAN
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extra-material">
   Extra Material
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hashing-using-a-binary-autoencoder">
     Hashing Using a Binary Autoencoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-solutions">
   Exercise Solutions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#to-8">
     1. to 8.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     9.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     10.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     11.
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Chapter 17 – Autoencoders and GANs**</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Chapter 17 – Autoencoders and GANs**
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-with-a-linear-autoencoder">
   PCA with a linear Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-autoencoders">
   Stacked Autoencoders
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-all-layers-at-once">
     Train all layers at once
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualizing-fashion-mnist">
   Visualizing Fashion MNIST
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tying-weights">
     Tying weights
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-one-autoencoder-at-a-time">
     Training one Autoencoder at a Time
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-convolutional-layers-instead-of-dense-layers">
     Using Convolutional Layers Instead of Dense Layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recurrent-autoencoders">
   Recurrent Autoencoders
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacked-denoising-autoencoder">
   Stacked denoising Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-autoencoder">
   Sparse Autoencoder
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-autoencoder">
   Variational Autoencoder
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-fashion-images">
     Generate Fashion Images
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-adversarial-networks">
   Generative Adversarial Networks
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-convolutional-gan">
   Deep Convolutional GAN
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extra-material">
   Extra Material
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hashing-using-a-binary-autoencoder">
     Hashing Using a Binary Autoencoder
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-solutions">
   Exercise Solutions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#to-8">
     1. to 8.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     9.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     10.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     11.
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-17-autoencoders-and-gans">
<h1>Chapter 17 – Autoencoders and GANs**<a class="headerlink" href="#chapter-17-autoencoders-and-gans" title="Permalink to this headline">#</a></h1>
<p><em>This notebook contains all the sample code in chapter 17.</em></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">#</a></h1>
<p>First, let’s import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Python ≥3.5 is required
import sys
assert sys.version_info &gt;= (3, 5)

# Is this notebook running on Colab or Kaggle?
IS_COLAB = &quot;google.colab&quot; in sys.modules
IS_KAGGLE = &quot;kaggle_secrets&quot; in sys.modules

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ &gt;= &quot;0.20&quot;

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ &gt;= &quot;2.0&quot;

if not tf.config.list_physical_devices(&#39;GPU&#39;):
    print(&quot;No GPU was detected. LSTMs and CNNs can be very slow without a GPU.&quot;)
    if IS_COLAB:
        print(&quot;Go to Runtime &gt; Change runtime and select a GPU hardware accelerator.&quot;)
    if IS_KAGGLE:
        print(&quot;Go to Settings &gt; Accelerator and select GPU.&quot;)

# Common imports
import numpy as np
import os

# to make this notebook&#39;s output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc(&#39;axes&#39;, labelsize=14)
mpl.rc(&#39;xtick&#39;, labelsize=12)
mpl.rc(&#39;ytick&#39;, labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = &quot;.&quot;
CHAPTER_ID = &quot;autoencoders&quot;
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, &quot;images&quot;, CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension=&quot;png&quot;, resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + &quot;.&quot; + fig_extension)
    print(&quot;Saving figure&quot;, fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No GPU was detected. LSTMs and CNNs can be very slow without a GPU.
</pre></div>
</div>
</div>
</div>
<p>A couple utility functions to plot grayscale 28x28 image:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_image(image):
    plt.imshow(image, cmap=&quot;binary&quot;)
    plt.axis(&quot;off&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="pca-with-a-linear-autoencoder">
<h1>PCA with a linear Autoencoder<a class="headerlink" href="#pca-with-a-linear-autoencoder" title="Permalink to this headline">#</a></h1>
<p>Build 3D dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(4)

def generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):
    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5
    data = np.empty((m, 3))
    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2
    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2
    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)
    return data

X_train = generate_3d_data(60)
X_train = X_train - X_train.mean(axis=0, keepdims=0)
</pre></div>
</div>
</div>
</div>
<p>Now let’s build the Autoencoder…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(42)
tf.random.set_seed(42)

encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])
decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])
autoencoder = keras.models.Sequential([encoder, decoder])

autoencoder.compile(loss=&quot;mse&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.5))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>history = autoencoder.fit(X_train, X_train, epochs=20)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 60 samples
Epoch 1/20
60/60 [==============================] - 0s 1ms/sample - loss: 0.2648
Epoch 2/20
60/60 [==============================] - 0s 49us/sample - loss: 0.1317
Epoch 3/20
60/60 [==============================] - 0s 50us/sample - loss: 0.0778
Epoch 4/20
60/60 [==============================] - 0s 46us/sample - loss: 0.0655
Epoch 5/20
60/60 [==============================] - 0s 51us/sample - loss: 0.0748
Epoch 6/20
60/60 [==============================] - 0s 47us/sample - loss: 0.1039
Epoch 7/20
60/60 [==============================] - 0s 50us/sample - loss: 0.1262
Epoch 8/20
60/60 [==============================] - 0s 52us/sample - loss: 0.0536
Epoch 9/20
60/60 [==============================] - 0s 51us/sample - loss: 0.0208
Epoch 10/20
60/60 [==============================] - 0s 52us/sample - loss: 0.0146
Epoch 11/20
60/60 [==============================] - 0s 52us/sample - loss: 0.0097
Epoch 12/20
60/60 [==============================] - 0s 48us/sample - loss: 0.0076
Epoch 13/20
60/60 [==============================] - 0s 43us/sample - loss: 0.0067
Epoch 14/20
60/60 [==============================] - 0s 49us/sample - loss: 0.0070
Epoch 15/20
60/60 [==============================] - 0s 58us/sample - loss: 0.0061
Epoch 16/20
60/60 [==============================] - 0s 53us/sample - loss: 0.0055
Epoch 17/20
60/60 [==============================] - 0s 63us/sample - loss: 0.0056
Epoch 18/20
60/60 [==============================] - 0s 58us/sample - loss: 0.0055
Epoch 19/20
60/60 [==============================] - 0s 64us/sample - loss: 0.0054
Epoch 20/20
60/60 [==============================] - 0s 55us/sample - loss: 0.0055
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>codings = encoder.predict(X_train)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig = plt.figure(figsize=(4,3))
plt.plot(codings[:,0], codings[:, 1], &quot;b.&quot;)
plt.xlabel(&quot;$z_1$&quot;, fontsize=18)
plt.ylabel(&quot;$z_2$&quot;, fontsize=18, rotation=0)
plt.grid(True)
save_fig(&quot;linear_autoencoder_pca_plot&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure linear_autoencoder_pca_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_14_1.png" src="../../_images/17_autoencoders_and_gans_14_1.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="stacked-autoencoders">
<h1>Stacked Autoencoders<a class="headerlink" href="#stacked-autoencoders" title="Permalink to this headline">#</a></h1>
<p>Let’s use MNIST:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full.astype(np.float32) / 255
X_test = X_test.astype(np.float32) / 255
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]
</pre></div>
</div>
</div>
</div>
<section id="train-all-layers-at-once">
<h2>Train all layers at once<a class="headerlink" href="#train-all-layers-at-once" title="Permalink to this headline">#</a></h2>
<p>Let’s build a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def rounded_accuracy(y_true, y_pred):
    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

stacked_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(30, activation=&quot;selu&quot;),
])
stacked_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])
stacked_ae.compile(loss=&quot;binary_crossentropy&quot;,
                   optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])
history = stacked_ae.fit(X_train, X_train, epochs=20,
                         validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/20
55000/55000 [==============================] - 4s 72us/sample - loss: 0.3386 - rounded_accuracy: 0.8866 - val_loss: 0.3118 - val_rounded_accuracy: 0.9128
Epoch 2/20
55000/55000 [==============================] - 4s 64us/sample - loss: 0.3055 - rounded_accuracy: 0.9153 - val_loss: 0.3030 - val_rounded_accuracy: 0.9200
Epoch 3/20
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2986 - rounded_accuracy: 0.9214 - val_loss: 0.2982 - val_rounded_accuracy: 0.9249
Epoch 4/20
55000/55000 [==============================] - 4s 67us/sample - loss: 0.2946 - rounded_accuracy: 0.9251 - val_loss: 0.2938 - val_rounded_accuracy: 0.9284
Epoch 5/20
55000/55000 [==============================] - 4s 70us/sample - loss: 0.2921 - rounded_accuracy: 0.9273 - val_loss: 0.2922 - val_rounded_accuracy: 0.9302
Epoch 6/20
55000/55000 [==============================] - 4s 69us/sample - loss: 0.2904 - rounded_accuracy: 0.9289 - val_loss: 0.2917 - val_rounded_accuracy: 0.9304
Epoch 7/20
55000/55000 [==============================] - 4s 72us/sample - loss: 0.2889 - rounded_accuracy: 0.9303 - val_loss: 0.2901 - val_rounded_accuracy: 0.9313
Epoch 8/20
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2878 - rounded_accuracy: 0.9311 - val_loss: 0.2884 - val_rounded_accuracy: 0.9324
Epoch 9/20
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2869 - rounded_accuracy: 0.9319 - val_loss: 0.2879 - val_rounded_accuracy: 0.9321
Epoch 10/20
55000/55000 [==============================] - 4s 69us/sample - loss: 0.2860 - rounded_accuracy: 0.9326 - val_loss: 0.2874 - val_rounded_accuracy: 0.9328
Epoch 11/20
55000/55000 [==============================] - 4s 71us/sample - loss: 0.2854 - rounded_accuracy: 0.9331 - val_loss: 0.2873 - val_rounded_accuracy: 0.9313
Epoch 12/20
55000/55000 [==============================] - 4s 72us/sample - loss: 0.2847 - rounded_accuracy: 0.9336 - val_loss: 0.2872 - val_rounded_accuracy: 0.9299
Epoch 13/20
55000/55000 [==============================] - 4s 65us/sample - loss: 0.2841 - rounded_accuracy: 0.9341 - val_loss: 0.2863 - val_rounded_accuracy: 0.9311
Epoch 14/20
55000/55000 [==============================] - 4s 67us/sample - loss: 0.2837 - rounded_accuracy: 0.9344 - val_loss: 0.2846 - val_rounded_accuracy: 0.9348
Epoch 15/20
55000/55000 [==============================] - 4s 65us/sample - loss: 0.2832 - rounded_accuracy: 0.9348 - val_loss: 0.2842 - val_rounded_accuracy: 0.9344
Epoch 16/20
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2827 - rounded_accuracy: 0.9352 - val_loss: 0.2850 - val_rounded_accuracy: 0.9359
Epoch 17/20
55000/55000 [==============================] - 4s 65us/sample - loss: 0.2823 - rounded_accuracy: 0.9355 - val_loss: 0.2841 - val_rounded_accuracy: 0.9363
Epoch 18/20
55000/55000 [==============================] - 4s 65us/sample - loss: 0.2820 - rounded_accuracy: 0.9357 - val_loss: 0.2832 - val_rounded_accuracy: 0.9355
Epoch 19/20
55000/55000 [==============================] - 4s 71us/sample - loss: 0.2817 - rounded_accuracy: 0.9360 - val_loss: 0.2858 - val_rounded_accuracy: 0.9361
Epoch 20/20
55000/55000 [==============================] - 4s 76us/sample - loss: 0.2814 - rounded_accuracy: 0.9363 - val_loss: 0.2835 - val_rounded_accuracy: 0.9370
</pre></div>
</div>
</div>
</div>
<p>This function processes a few test images through the autoencoder and displays the original images and their reconstructions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def show_reconstructions(model, images=X_valid, n_images=5):
    reconstructions = model.predict(images[:n_images])
    fig = plt.figure(figsize=(n_images * 1.5, 3))
    for image_index in range(n_images):
        plt.subplot(2, n_images, 1 + image_index)
        plot_image(images[image_index])
        plt.subplot(2, n_images, 1 + n_images + image_index)
        plot_image(reconstructions[image_index])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(stacked_ae)
save_fig(&quot;reconstruction_plot&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure reconstruction_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_24_1.png" src="../../_images/17_autoencoders_and_gans_24_1.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="visualizing-fashion-mnist">
<h1>Visualizing Fashion MNIST<a class="headerlink" href="#visualizing-fashion-mnist" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(42)

from sklearn.manifold import TSNE

X_valid_compressed = stacked_encoder.predict(X_valid)
tsne = TSNE()
X_valid_2D = tsne.fit_transform(X_valid_compressed)
X_valid_2D = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=&quot;tab10&quot;)
plt.axis(&quot;off&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_27_0.png" src="../../_images/17_autoencoders_and_gans_27_0.png" />
</div>
</div>
<p>Let’s make this diagram a bit prettier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
plt.figure(figsize=(10, 8))
cmap = plt.cm.tab10
plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=cmap)
image_positions = np.array([[1., 1.]])
for index, position in enumerate(X_valid_2D):
    dist = np.sum((position - image_positions) ** 2, axis=1)
    if np.min(dist) &gt; 0.02: # if far enough from other images
        image_positions = np.r_[image_positions, [position]]
        imagebox = mpl.offsetbox.AnnotationBbox(
            mpl.offsetbox.OffsetImage(X_valid[index], cmap=&quot;binary&quot;),
            position, bboxprops={&quot;edgecolor&quot;: cmap(y_valid[index]), &quot;lw&quot;: 2})
        plt.gca().add_artist(imagebox)
plt.axis(&quot;off&quot;)
save_fig(&quot;fashion_mnist_visualization_plot&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure fashion_mnist_visualization_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_29_1.png" src="../../_images/17_autoencoders_and_gans_29_1.png" />
</div>
</div>
<section id="tying-weights">
<h2>Tying weights<a class="headerlink" href="#tying-weights" title="Permalink to this headline">#</a></h2>
<p>It is common to tie the weights of the encoder and the decoder, by simply using the transpose of the encoder’s weights as the decoder weights. For this, we need to use a custom layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class DenseTranspose(keras.layers.Layer):
    def __init__(self, dense, activation=None, **kwargs):
        self.dense = dense
        self.activation = keras.activations.get(activation)
        super().__init__(**kwargs)
    def build(self, batch_input_shape):
        self.biases = self.add_weight(name=&quot;bias&quot;,
                                      shape=[self.dense.input_shape[-1]],
                                      initializer=&quot;zeros&quot;)
        super().build(batch_input_shape)
    def call(self, inputs):
        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)
        return self.activation(z + self.biases)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

dense_1 = keras.layers.Dense(100, activation=&quot;selu&quot;)
dense_2 = keras.layers.Dense(30, activation=&quot;selu&quot;)

tied_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    dense_1,
    dense_2
])

tied_decoder = keras.models.Sequential([
    DenseTranspose(dense_2, activation=&quot;selu&quot;),
    DenseTranspose(dense_1, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])

tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])

tied_ae.compile(loss=&quot;binary_crossentropy&quot;,
                optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])
history = tied_ae.fit(X_train, X_train, epochs=10,
                      validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 4s 80us/sample - loss: 0.3213 - rounded_accuracy: 0.8996 - val_loss: 0.3038 - val_rounded_accuracy: 0.9154
Epoch 2/10
55000/55000 [==============================] - 4s 74us/sample - loss: 0.2967 - rounded_accuracy: 0.9216 - val_loss: 0.2931 - val_rounded_accuracy: 0.9268
Epoch 3/10
55000/55000 [==============================] - 4s 70us/sample - loss: 0.2916 - rounded_accuracy: 0.9263 - val_loss: 0.2929 - val_rounded_accuracy: 0.9254
Epoch 4/10
55000/55000 [==============================] - 4s 64us/sample - loss: 0.2889 - rounded_accuracy: 0.9287 - val_loss: 0.2905 - val_rounded_accuracy: 0.9316
Epoch 5/10
55000/55000 [==============================] - 4s 70us/sample - loss: 0.2871 - rounded_accuracy: 0.9303 - val_loss: 0.2917 - val_rounded_accuracy: 0.9307
Epoch 6/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2858 - rounded_accuracy: 0.9316 - val_loss: 0.2870 - val_rounded_accuracy: 0.9332
Epoch 7/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2847 - rounded_accuracy: 0.9327 - val_loss: 0.2865 - val_rounded_accuracy: 0.9336
Epoch 8/10
55000/55000 [==============================] - 4s 71us/sample - loss: 0.2840 - rounded_accuracy: 0.9334 - val_loss: 0.2859 - val_rounded_accuracy: 0.9349
Epoch 9/10
55000/55000 [==============================] - 4s 70us/sample - loss: 0.2834 - rounded_accuracy: 0.9339 - val_loss: 0.2864 - val_rounded_accuracy: 0.9338
Epoch 10/10
55000/55000 [==============================] - 4s 72us/sample - loss: 0.2828 - rounded_accuracy: 0.9345 - val_loss: 0.2839 - val_rounded_accuracy: 0.9338
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(tied_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_34_0.png" src="../../_images/17_autoencoders_and_gans_34_0.png" />
</div>
</div>
</section>
<section id="training-one-autoencoder-at-a-time">
<h2>Training one Autoencoder at a Time<a class="headerlink" href="#training-one-autoencoder-at-a-time" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train_autoencoder(n_neurons, X_train, X_valid, loss, optimizer,
                      n_epochs=10, output_activation=None, metrics=None):
    n_inputs = X_train.shape[-1]
    encoder = keras.models.Sequential([
        keras.layers.Dense(n_neurons, activation=&quot;selu&quot;, input_shape=[n_inputs])
    ])
    decoder = keras.models.Sequential([
        keras.layers.Dense(n_inputs, activation=output_activation),
    ])
    autoencoder = keras.models.Sequential([encoder, decoder])
    autoencoder.compile(optimizer, loss, metrics=metrics)
    autoencoder.fit(X_train, X_train, epochs=n_epochs,
                    validation_data=(X_valid, X_valid))
    return encoder, decoder, encoder(X_train), encoder(X_valid)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

K = keras.backend
X_train_flat = K.batch_flatten(X_train) # equivalent to .reshape(-1, 28 * 28)
X_valid_flat = K.batch_flatten(X_valid)
enc1, dec1, X_train_enc1, X_valid_enc1 = train_autoencoder(
    100, X_train_flat, X_valid_flat, &quot;binary_crossentropy&quot;,
    keras.optimizers.SGD(learning_rate=1.5), output_activation=&quot;sigmoid&quot;,
    metrics=[rounded_accuracy])
enc2, dec2, _, _ = train_autoencoder(
    30, X_train_enc1, X_valid_enc1, &quot;mse&quot;, keras.optimizers.SGD(learning_rate=0.05),
    output_activation=&quot;selu&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 4s 73us/sample - loss: 0.3446 - rounded_accuracy: 0.8874 - val_loss: 0.3122 - val_rounded_accuracy: 0.9147
Epoch 2/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3039 - rounded_accuracy: 0.9204 - val_loss: 0.3006 - val_rounded_accuracy: 0.9241
Epoch 3/10
55000/55000 [==============================] - 4s 69us/sample - loss: 0.2949 - rounded_accuracy: 0.9286 - val_loss: 0.2933 - val_rounded_accuracy: 0.9319
Epoch 4/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2890 - rounded_accuracy: 0.9343 - val_loss: 0.2887 - val_rounded_accuracy: 0.9362
Epoch 5/10
55000/55000 [==============================] - 4s 72us/sample - loss: 0.2853 - rounded_accuracy: 0.9379 - val_loss: 0.2856 - val_rounded_accuracy: 0.9390
Epoch 6/10
55000/55000 [==============================] - 4s 67us/sample - loss: 0.2826 - rounded_accuracy: 0.9404 - val_loss: 0.2833 - val_rounded_accuracy: 0.9410
Epoch 7/10
55000/55000 [==============================] - 4s 69us/sample - loss: 0.2806 - rounded_accuracy: 0.9424 - val_loss: 0.2816 - val_rounded_accuracy: 0.9430
Epoch 8/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2791 - rounded_accuracy: 0.9439 - val_loss: 0.2802 - val_rounded_accuracy: 0.9448
Epoch 9/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.2778 - rounded_accuracy: 0.9451 - val_loss: 0.2790 - val_rounded_accuracy: 0.9454
Epoch 10/10
55000/55000 [==============================] - 4s 65us/sample - loss: 0.2768 - rounded_accuracy: 0.9461 - val_loss: 0.2781 - val_rounded_accuracy: 0.9462
Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 2s 35us/sample - loss: 0.5678 - val_loss: 0.2887
Epoch 2/10
55000/55000 [==============================] - 2s 30us/sample - loss: 0.2633 - val_loss: 0.2512
Epoch 3/10
55000/55000 [==============================] - 2s 33us/sample - loss: 0.2237 - val_loss: 0.2115
Epoch 4/10
55000/55000 [==============================] - 2s 33us/sample - loss: 0.2025 - val_loss: 0.1967
Epoch 5/10
55000/55000 [==============================] - 2s 31us/sample - loss: 0.1909 - val_loss: 0.1864
Epoch 6/10
55000/55000 [==============================] - 2s 29us/sample - loss: 0.1824 - val_loss: 0.1734
Epoch 7/10
55000/55000 [==============================] - 2s 31us/sample - loss: 0.1750 - val_loss: 0.1696
Epoch 8/10
55000/55000 [==============================] - 2s 31us/sample - loss: 0.1732 - val_loss: 0.1719
Epoch 9/10
55000/55000 [==============================] - 2s 30us/sample - loss: 0.1711 - val_loss: 0.1917
Epoch 10/10
55000/55000 [==============================] - 2s 29us/sample - loss: 0.1704 - val_loss: 0.1687
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>stacked_ae_1_by_1 = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    enc1, enc2, dec2, dec1,
    keras.layers.Reshape([28, 28])
])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(stacked_ae_1_by_1)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_39_0.png" src="../../_images/17_autoencoders_and_gans_39_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>stacked_ae_1_by_1.compile(loss=&quot;binary_crossentropy&quot;,
                          optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=[rounded_accuracy])
history = stacked_ae_1_by_1.fit(X_train, X_train, epochs=10,
                                validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 83us/sample - loss: 0.2853 - rounded_accuracy: 0.9359 - val_loss: 0.2868 - val_rounded_accuracy: 0.9361
Epoch 2/10
55000/55000 [==============================] - 4s 70us/sample - loss: 0.2849 - rounded_accuracy: 0.9363 - val_loss: 0.2866 - val_rounded_accuracy: 0.9364
Epoch 3/10
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2847 - rounded_accuracy: 0.9365 - val_loss: 0.2864 - val_rounded_accuracy: 0.9362
Epoch 4/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.2846 - rounded_accuracy: 0.9366 - val_loss: 0.2863 - val_rounded_accuracy: 0.9367
Epoch 5/10
55000/55000 [==============================] - 4s 74us/sample - loss: 0.2844 - rounded_accuracy: 0.9368 - val_loss: 0.2862 - val_rounded_accuracy: 0.9369
Epoch 6/10
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2843 - rounded_accuracy: 0.9369 - val_loss: 0.2861 - val_rounded_accuracy: 0.9368
Epoch 7/10
55000/55000 [==============================] - 4s 67us/sample - loss: 0.2842 - rounded_accuracy: 0.9370 - val_loss: 0.2860 - val_rounded_accuracy: 0.9368
Epoch 8/10
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2841 - rounded_accuracy: 0.9371 - val_loss: 0.2859 - val_rounded_accuracy: 0.9369
Epoch 9/10
55000/55000 [==============================] - 4s 67us/sample - loss: 0.2840 - rounded_accuracy: 0.9372 - val_loss: 0.2858 - val_rounded_accuracy: 0.9368
Epoch 10/10
55000/55000 [==============================] - 4s 66us/sample - loss: 0.2839 - rounded_accuracy: 0.9373 - val_loss: 0.2857 - val_rounded_accuracy: 0.9371
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(stacked_ae_1_by_1)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_41_0.png" src="../../_images/17_autoencoders_and_gans_41_0.png" />
</div>
</div>
</section>
<section id="using-convolutional-layers-instead-of-dense-layers">
<h2>Using Convolutional Layers Instead of Dense Layers<a class="headerlink" href="#using-convolutional-layers-instead-of-dense-layers" title="Permalink to this headline">#</a></h2>
<p>Let’s build a stacked Autoencoder with 3 hidden layers and 1 output layer (i.e., 2 stacked Autoencoders).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

conv_encoder = keras.models.Sequential([
    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),
    keras.layers.Conv2D(16, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(32, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;),
    keras.layers.MaxPool2D(pool_size=2),
    keras.layers.Conv2D(64, kernel_size=3, padding=&quot;SAME&quot;, activation=&quot;selu&quot;),
    keras.layers.MaxPool2D(pool_size=2)
])
conv_decoder = keras.models.Sequential([
    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=&quot;VALID&quot;, activation=&quot;selu&quot;,
                                 input_shape=[3, 3, 64]),
    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=&quot;SAME&quot;, activation=&quot;selu&quot;),
    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=&quot;SAME&quot;, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])

conv_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.0),
                metrics=[rounded_accuracy])
history = conv_ae.fit(X_train, X_train, epochs=5,
                      validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/5
55000/55000 [==============================] - 40s 734us/sample - loss: 0.3017 - accuracy: 0.5064 - val_loss: 0.2842 - val_accuracy: 0.5058
Epoch 2/5
55000/55000 [==============================] - 39s 712us/sample - loss: 0.2756 - accuracy: 0.5088 - val_loss: 0.2739 - val_accuracy: 0.5058
Epoch 3/5
55000/55000 [==============================] - 39s 715us/sample - loss: 0.2709 - accuracy: 0.5092 - val_loss: 0.2720 - val_accuracy: 0.5059
Epoch 4/5
55000/55000 [==============================] - 39s 707us/sample - loss: 0.2682 - accuracy: 0.5094 - val_loss: 0.2685 - val_accuracy: 0.5063
Epoch 5/5
55000/55000 [==============================] - 39s 706us/sample - loss: 0.2665 - accuracy: 0.5095 - val_loss: 0.2671 - val_accuracy: 0.5066
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>conv_encoder.summary()
conv_decoder.summary()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape_3 (Reshape)          (None, 28, 28, 1)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 28, 28, 16)        160       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         
=================================================================
Total params: 23,296
Trainable params: 23,296
Non-trainable params: 0
_________________________________________________________________
Model: &quot;sequential_17&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_transpose (Conv2DTran (None, 7, 7, 32)          18464     
_________________________________________________________________
conv2d_transpose_1 (Conv2DTr (None, 14, 14, 16)        4624      
_________________________________________________________________
conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         145       
_________________________________________________________________
reshape_4 (Reshape)          (None, 28, 28)            0         
=================================================================
Total params: 23,233
Trainable params: 23,233
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(conv_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_46_0.png" src="../../_images/17_autoencoders_and_gans_46_0.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="recurrent-autoencoders">
<h1>Recurrent Autoencoders<a class="headerlink" href="#recurrent-autoencoders" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>recurrent_encoder = keras.models.Sequential([
    keras.layers.LSTM(100, return_sequences=True, input_shape=[28, 28]),
    keras.layers.LSTM(30)
])
recurrent_decoder = keras.models.Sequential([
    keras.layers.RepeatVector(28, input_shape=[30]),
    keras.layers.LSTM(100, return_sequences=True),
    keras.layers.TimeDistributed(keras.layers.Dense(28, activation=&quot;sigmoid&quot;))
])
recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])
recurrent_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(0.1),
                     metrics=[rounded_accuracy])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>history = recurrent_ae.fit(X_train, X_train, epochs=10, validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 79s 1ms/sample - loss: 0.5165 - rounded_accuracy: 0.7363 - val_loss: 0.4489 - val_rounded_accuracy: 0.8137
Epoch 2/10
55000/55000 [==============================] - 78s 1ms/sample - loss: 0.4049 - rounded_accuracy: 0.8415 - val_loss: 0.3762 - val_rounded_accuracy: 0.8650
Epoch 3/10
55000/55000 [==============================] - 80s 1ms/sample - loss: 0.3662 - rounded_accuracy: 0.8703 - val_loss: 0.3626 - val_rounded_accuracy: 0.8730
Epoch 4/10
55000/55000 [==============================] - 80s 1ms/sample - loss: 0.3505 - rounded_accuracy: 0.8808 - val_loss: 0.3483 - val_rounded_accuracy: 0.8838
Epoch 5/10
55000/55000 [==============================] - 82s 1ms/sample - loss: 0.3398 - rounded_accuracy: 0.8881 - val_loss: 0.3345 - val_rounded_accuracy: 0.8941
Epoch 6/10
55000/55000 [==============================] - 93s 2ms/sample - loss: 0.3328 - rounded_accuracy: 0.8930 - val_loss: 0.3372 - val_rounded_accuracy: 0.8914
Epoch 7/10
55000/55000 [==============================] - 94s 2ms/sample - loss: 0.3280 - rounded_accuracy: 0.8962 - val_loss: 0.3261 - val_rounded_accuracy: 0.8980
Epoch 8/10
55000/55000 [==============================] - 95s 2ms/sample - loss: 0.3244 - rounded_accuracy: 0.8988 - val_loss: 0.3226 - val_rounded_accuracy: 0.9030
Epoch 9/10
55000/55000 [==============================] - 92s 2ms/sample - loss: 0.3215 - rounded_accuracy: 0.9010 - val_loss: 0.3239 - val_rounded_accuracy: 0.8958
Epoch 10/10
55000/55000 [==============================] - 90s 2ms/sample - loss: 0.3190 - rounded_accuracy: 0.9030 - val_loss: 0.3206 - val_rounded_accuracy: 0.9015
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tensorflow.python.keras.callbacks.History at 0x1a5b98fa20&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(recurrent_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_50_0.png" src="../../_images/17_autoencoders_and_gans_50_0.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="stacked-denoising-autoencoder">
<h1>Stacked denoising Autoencoder<a class="headerlink" href="#stacked-denoising-autoencoder" title="Permalink to this headline">#</a></h1>
<p>Using Gaussian noise:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

denoising_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.GaussianNoise(0.2),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(30, activation=&quot;selu&quot;)
])
denoising_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
denoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])
denoising_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.0),
                     metrics=[rounded_accuracy])
history = denoising_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 82us/sample - loss: 0.3508 - rounded_accuracy: 0.8768 - val_loss: 0.3231 - val_rounded_accuracy: 0.9065
Epoch 2/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.3125 - rounded_accuracy: 0.9093 - val_loss: 0.3077 - val_rounded_accuracy: 0.9153
Epoch 3/10
55000/55000 [==============================] - 4s 74us/sample - loss: 0.3061 - rounded_accuracy: 0.9149 - val_loss: 0.3034 - val_rounded_accuracy: 0.9190
Epoch 4/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.3025 - rounded_accuracy: 0.9181 - val_loss: 0.3007 - val_rounded_accuracy: 0.9195
Epoch 5/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.2998 - rounded_accuracy: 0.9203 - val_loss: 0.2980 - val_rounded_accuracy: 0.9230
Epoch 6/10
55000/55000 [==============================] - 4s 78us/sample - loss: 0.2979 - rounded_accuracy: 0.9220 - val_loss: 0.2987 - val_rounded_accuracy: 0.9193
Epoch 7/10
55000/55000 [==============================] - 4s 74us/sample - loss: 0.2965 - rounded_accuracy: 0.9233 - val_loss: 0.2945 - val_rounded_accuracy: 0.9269
Epoch 8/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.2953 - rounded_accuracy: 0.9243 - val_loss: 0.2946 - val_rounded_accuracy: 0.9286
Epoch 9/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.2943 - rounded_accuracy: 0.9251 - val_loss: 0.2927 - val_rounded_accuracy: 0.9283
Epoch 10/10
55000/55000 [==============================] - 4s 77us/sample - loss: 0.2935 - rounded_accuracy: 0.9258 - val_loss: 0.2920 - val_rounded_accuracy: 0.9291
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

noise = keras.layers.GaussianNoise(0.2)
show_reconstructions(denoising_ae, noise(X_valid, training=True))
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_54_0.png" src="../../_images/17_autoencoders_and_gans_54_0.png" />
</div>
</div>
<p>Using dropout:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

dropout_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(30, activation=&quot;selu&quot;)
])
dropout_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])
dropout_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.0),
                   metrics=[rounded_accuracy])
history = dropout_ae.fit(X_train, X_train, epochs=10,
                         validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 83us/sample - loss: 0.3564 - accuracy: 0.4969 - val_loss: 0.3206 - val_accuracy: 0.5011
Epoch 2/10
55000/55000 [==============================] - 4s 73us/sample - loss: 0.3182 - accuracy: 0.5034 - val_loss: 0.3113 - val_accuracy: 0.5014
Epoch 3/10
55000/55000 [==============================] - 4s 74us/sample - loss: 0.3130 - accuracy: 0.5042 - val_loss: 0.3079 - val_accuracy: 0.5012
Epoch 4/10
55000/55000 [==============================] - 4s 73us/sample - loss: 0.3091 - accuracy: 0.5048 - val_loss: 0.3037 - val_accuracy: 0.5026
Epoch 5/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3066 - accuracy: 0.5052 - val_loss: 0.3032 - val_accuracy: 0.5016
Epoch 6/10
55000/55000 [==============================] - 4s 78us/sample - loss: 0.3047 - accuracy: 0.5054 - val_loss: 0.3001 - val_accuracy: 0.5032
Epoch 7/10
55000/55000 [==============================] - 4s 79us/sample - loss: 0.3033 - accuracy: 0.5056 - val_loss: 0.2987 - val_accuracy: 0.5033
Epoch 8/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3021 - accuracy: 0.5057 - val_loss: 0.2976 - val_accuracy: 0.5033
Epoch 9/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.3012 - accuracy: 0.5058 - val_loss: 0.2976 - val_accuracy: 0.5033
Epoch 10/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3004 - accuracy: 0.5059 - val_loss: 0.2958 - val_accuracy: 0.5033
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

dropout = keras.layers.Dropout(0.5)
show_reconstructions(dropout_ae, dropout(X_valid, training=True))
save_fig(&quot;dropout_denoising_plot&quot;, tight_layout=False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure dropout_denoising_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_57_1.png" src="../../_images/17_autoencoders_and_gans_57_1.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="sparse-autoencoder">
<h1>Sparse Autoencoder<a class="headerlink" href="#sparse-autoencoder" title="Permalink to this headline">#</a></h1>
<p>Let’s build a simple stacked autoencoder, so we can compare it to the sparse autoencoders we will build. This time we will use the sigmoid activation function for the coding layer, to ensure that the coding values range from 0 to 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

simple_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(30, activation=&quot;sigmoid&quot;),
])
simple_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
simple_ae = keras.models.Sequential([simple_encoder, simple_decoder])
simple_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.),
                  metrics=[rounded_accuracy])
history = simple_ae.fit(X_train, X_train, epochs=10,
                        validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 4s 78us/sample - loss: 0.4331 - accuracy: 0.4906 - val_loss: 0.3778 - val_accuracy: 0.4911
Epoch 2/10
55000/55000 [==============================] - 4s 67us/sample - loss: 0.3610 - accuracy: 0.4976 - val_loss: 0.3510 - val_accuracy: 0.4972
Epoch 3/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3405 - accuracy: 0.5006 - val_loss: 0.3359 - val_accuracy: 0.4990
Epoch 4/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3276 - accuracy: 0.5027 - val_loss: 0.3248 - val_accuracy: 0.5003
Epoch 5/10
55000/55000 [==============================] - 4s 72us/sample - loss: 0.3206 - accuracy: 0.5035 - val_loss: 0.3206 - val_accuracy: 0.5007
Epoch 6/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3172 - accuracy: 0.5038 - val_loss: 0.3176 - val_accuracy: 0.5010
Epoch 7/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3149 - accuracy: 0.5041 - val_loss: 0.3154 - val_accuracy: 0.5013
Epoch 8/10
55000/55000 [==============================] - 4s 69us/sample - loss: 0.3128 - accuracy: 0.5045 - val_loss: 0.3133 - val_accuracy: 0.5014
Epoch 9/10
55000/55000 [==============================] - 4s 68us/sample - loss: 0.3108 - accuracy: 0.5049 - val_loss: 0.3118 - val_accuracy: 0.5023
Epoch 10/10
55000/55000 [==============================] - 4s 71us/sample - loss: 0.3088 - accuracy: 0.5053 - val_loss: 0.3092 - val_accuracy: 0.5023
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(simple_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_61_0.png" src="../../_images/17_autoencoders_and_gans_61_0.png" />
</div>
</div>
<p>Let’s create a couple functions to print nice activation histograms:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_percent_hist(ax, data, bins):
    counts, _ = np.histogram(data, bins=bins)
    widths = bins[1:] - bins[:-1]
    x = bins[:-1] + widths / 2
    ax.bar(x, counts / len(data), width=widths*0.8)
    ax.xaxis.set_ticks(bins)
    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(
        lambda y, position: &quot;{}%&quot;.format(int(np.round(100 * y)))))
    ax.grid(True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_activations_histogram(encoder, height=1, n_bins=10):
    X_valid_codings = encoder(X_valid).numpy()
    activation_means = X_valid_codings.mean(axis=0)
    mean = activation_means.mean()
    bins = np.linspace(0, 1, n_bins + 1)

    fig, [ax1, ax2] = plt.subplots(figsize=(10, 3), nrows=1, ncols=2, sharey=True)
    plot_percent_hist(ax1, X_valid_codings.ravel(), bins)
    ax1.plot([mean, mean], [0, height], &quot;k--&quot;, label=&quot;Overall Mean = {:.2f}&quot;.format(mean))
    ax1.legend(loc=&quot;upper center&quot;, fontsize=14)
    ax1.set_xlabel(&quot;Activation&quot;)
    ax1.set_ylabel(&quot;% Activations&quot;)
    ax1.axis([0, 1, 0, height])
    plot_percent_hist(ax2, activation_means, bins)
    ax2.plot([mean, mean], [0, height], &quot;k--&quot;)
    ax2.set_xlabel(&quot;Neuron Mean Activation&quot;)
    ax2.set_ylabel(&quot;% Neurons&quot;)
    ax2.axis([0, 1, 0, height])
</pre></div>
</div>
</div>
</div>
<p>Let’s use these functions to plot histograms of the activations of the encoding layer. The histogram on the left shows the distribution of all the activations. You can see that values close to 0 or 1 are more frequent overall, which is consistent with the saturating nature of the sigmoid function. The histogram on the right shows the distribution of mean neuron activations: you can see that most neurons have a mean activation close to 0.5. Both histograms tell us that each neuron tends to either fire close to 0 or 1, with about 50% probability each. However, some neurons fire almost all the time (right side of the right histogram).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_activations_histogram(simple_encoder, height=0.35)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_66_0.png" src="../../_images/17_autoencoders_and_gans_66_0.png" />
</div>
</div>
<p>Now let’s add <span class="math notranslate nohighlight">\(\ell_1\)</span> regularization to the coding layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

sparse_l1_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(300, activation=&quot;sigmoid&quot;),
    keras.layers.ActivityRegularization(l1=1e-3)  # Alternatively, you could add
                                                  # activity_regularizer=keras.regularizers.l1(1e-3)
                                                  # to the previous layer.
])
sparse_l1_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[300]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])
sparse_l1_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.0),
                     metrics=[rounded_accuracy])
history = sparse_l1_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 5s 98us/sample - loss: 0.4306 - accuracy: 0.4947 - val_loss: 0.3819 - val_accuracy: 0.4897
Epoch 2/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.3689 - accuracy: 0.4971 - val_loss: 0.3639 - val_accuracy: 0.4940
Epoch 3/10
55000/55000 [==============================] - 5s 86us/sample - loss: 0.3553 - accuracy: 0.4987 - val_loss: 0.3513 - val_accuracy: 0.4970
Epoch 4/10
55000/55000 [==============================] - 4s 78us/sample - loss: 0.3443 - accuracy: 0.5003 - val_loss: 0.3428 - val_accuracy: 0.4964
Epoch 5/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3379 - accuracy: 0.5009 - val_loss: 0.3372 - val_accuracy: 0.4979
Epoch 6/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3332 - accuracy: 0.5015 - val_loss: 0.3329 - val_accuracy: 0.4980
Epoch 7/10
55000/55000 [==============================] - 4s 78us/sample - loss: 0.3286 - accuracy: 0.5025 - val_loss: 0.3306 - val_accuracy: 0.4981
Epoch 8/10
55000/55000 [==============================] - 4s 76us/sample - loss: 0.3249 - accuracy: 0.5032 - val_loss: 0.3254 - val_accuracy: 0.5000
Epoch 9/10
55000/55000 [==============================] - 4s 80us/sample - loss: 0.3223 - accuracy: 0.5036 - val_loss: 0.3244 - val_accuracy: 0.4995
Epoch 10/10
55000/55000 [==============================] - 4s 75us/sample - loss: 0.3205 - accuracy: 0.5039 - val_loss: 0.3212 - val_accuracy: 0.5014
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(sparse_l1_ae)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_69_0.png" src="../../_images/17_autoencoders_and_gans_69_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_activations_histogram(sparse_l1_encoder, height=1.)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_70_0.png" src="../../_images/17_autoencoders_and_gans_70_0.png" />
</div>
</div>
<p>Let’s use the KL Divergence loss instead to ensure sparsity, and target 10% sparsity rather than 0%:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>p = 0.1
q = np.linspace(0.001, 0.999, 500)
kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))
mse = (p - q)**2
mae = np.abs(p - q)
plt.plot([p, p], [0, 0.3], &quot;k:&quot;)
plt.text(0.05, 0.32, &quot;Target\nsparsity&quot;, fontsize=14)
plt.plot(q, kl_div, &quot;b-&quot;, label=&quot;KL divergence&quot;)
plt.plot(q, mae, &quot;g--&quot;, label=r&quot;MAE ($\ell_1$)&quot;)
plt.plot(q, mse, &quot;r--&quot;, linewidth=1, label=r&quot;MSE ($\ell_2$)&quot;)
plt.legend(loc=&quot;upper left&quot;, fontsize=14)
plt.xlabel(&quot;Actual sparsity&quot;)
plt.ylabel(&quot;Cost&quot;, rotation=0)
plt.axis([0, 1, 0, 0.95])
save_fig(&quot;sparsity_loss_plot&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure sparsity_loss_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_72_1.png" src="../../_images/17_autoencoders_and_gans_72_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>K = keras.backend
kl_divergence = keras.losses.kullback_leibler_divergence

class KLDivergenceRegularizer(keras.regularizers.Regularizer):
    def __init__(self, weight, target=0.1):
        self.weight = weight
        self.target = target
    def __call__(self, inputs):
        mean_activities = K.mean(inputs, axis=0)
        return self.weight * (
            kl_divergence(self.target, mean_activities) +
            kl_divergence(1. - self.target, 1. - mean_activities))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)
sparse_kl_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(300, activation=&quot;sigmoid&quot;, activity_regularizer=kld_reg)
])
sparse_kl_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[300]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])
sparse_kl_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.SGD(learning_rate=1.0),
              metrics=[rounded_accuracy])
history = sparse_kl_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/10
55000/55000 [==============================] - 6s 103us/sample - loss: 0.4151 - rounded_accuracy: 0.8121 - val_loss: 0.3714 - val_rounded_accuracy: 0.8560
Epoch 2/10
55000/55000 [==============================] - 4s 81us/sample - loss: 0.3532 - rounded_accuracy: 0.8762 - val_loss: 0.3442 - val_rounded_accuracy: 0.8842
Epoch 3/10
55000/55000 [==============================] - 5s 83us/sample - loss: 0.3340 - rounded_accuracy: 0.8919 - val_loss: 0.3292 - val_rounded_accuracy: 0.8976
Epoch 4/10
55000/55000 [==============================] - 5s 84us/sample - loss: 0.3224 - rounded_accuracy: 0.9018 - val_loss: 0.3213 - val_rounded_accuracy: 0.9040
Epoch 5/10
55000/55000 [==============================] - 5s 85us/sample - loss: 0.3170 - rounded_accuracy: 0.9062 - val_loss: 0.3170 - val_rounded_accuracy: 0.9075
Epoch 6/10
55000/55000 [==============================] - 5s 82us/sample - loss: 0.3134 - rounded_accuracy: 0.9093 - val_loss: 0.3140 - val_rounded_accuracy: 0.9105
Epoch 7/10
55000/55000 [==============================] - 5s 85us/sample - loss: 0.3107 - rounded_accuracy: 0.9116 - val_loss: 0.3114 - val_rounded_accuracy: 0.9121
Epoch 8/10
55000/55000 [==============================] - 5s 83us/sample - loss: 0.3084 - rounded_accuracy: 0.9136 - val_loss: 0.3094 - val_rounded_accuracy: 0.9145
Epoch 9/10
55000/55000 [==============================] - 5s 83us/sample - loss: 0.3064 - rounded_accuracy: 0.9154 - val_loss: 0.3074 - val_rounded_accuracy: 0.9166
Epoch 10/10
55000/55000 [==============================] - 5s 84us/sample - loss: 0.3044 - rounded_accuracy: 0.9170 - val_loss: 0.3053 - val_rounded_accuracy: 0.9174
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(sparse_kl_ae)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_75_0.png" src="../../_images/17_autoencoders_and_gans_75_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_activations_histogram(sparse_kl_encoder)
save_fig(&quot;sparse_autoencoder_plot&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure sparse_autoencoder_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_76_1.png" src="../../_images/17_autoencoders_and_gans_76_1.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="variational-autoencoder">
<h1>Variational Autoencoder<a class="headerlink" href="#variational-autoencoder" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class Sampling(keras.layers.Layer):
    def call(self, inputs):
        mean, log_var = inputs
        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

codings_size = 10

inputs = keras.layers.Input(shape=[28, 28])
z = keras.layers.Flatten()(inputs)
z = keras.layers.Dense(150, activation=&quot;selu&quot;)(z)
z = keras.layers.Dense(100, activation=&quot;selu&quot;)(z)
codings_mean = keras.layers.Dense(codings_size)(z)
codings_log_var = keras.layers.Dense(codings_size)(z)
codings = Sampling()([codings_mean, codings_log_var])
variational_encoder = keras.models.Model(
    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])

decoder_inputs = keras.layers.Input(shape=[codings_size])
x = keras.layers.Dense(100, activation=&quot;selu&quot;)(decoder_inputs)
x = keras.layers.Dense(150, activation=&quot;selu&quot;)(x)
x = keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;)(x)
outputs = keras.layers.Reshape([28, 28])(x)
variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])

_, _, codings = variational_encoder(inputs)
reconstructions = variational_decoder(codings)
variational_ae = keras.models.Model(inputs=[inputs], outputs=[reconstructions])

latent_loss = -0.5 * K.sum(
    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),
    axis=-1)
variational_ae.add_loss(K.mean(latent_loss) / 784.)
variational_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;, metrics=[rounded_accuracy])
history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,
                             validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 55000 samples, validate on 5000 samples
Epoch 1/25
55000/55000 [==============================] - 5s 84us/sample - loss: 0.3889 - rounded_accuracy: 0.8608 - val_loss: 0.3592 - val_rounded_accuracy: 0.8840
Epoch 2/25
55000/55000 [==============================] - 3s 60us/sample - loss: 0.3429 - rounded_accuracy: 0.8974 - val_loss: 0.3369 - val_rounded_accuracy: 0.8982
Epoch 3/25
55000/55000 [==============================] - 3s 53us/sample - loss: 0.3329 - rounded_accuracy: 0.9050 - val_loss: 0.3356 - val_rounded_accuracy: 0.9022
Epoch 4/25
55000/55000 [==============================] - 3s 61us/sample - loss: 0.3275 - rounded_accuracy: 0.9092 - val_loss: 0.3255 - val_rounded_accuracy: 0.9105
Epoch 5/25
55000/55000 [==============================] - 3s 59us/sample - loss: 0.3243 - rounded_accuracy: 0.9119 - val_loss: 0.3232 - val_rounded_accuracy: 0.9169
Epoch 6/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3219 - rounded_accuracy: 0.9138 - val_loss: 0.3236 - val_rounded_accuracy: 0.9149
Epoch 7/25
55000/55000 [==============================] - 3s 55us/sample - loss: 0.3204 - rounded_accuracy: 0.9150 - val_loss: 0.3194 - val_rounded_accuracy: 0.9176
Epoch 8/25
55000/55000 [==============================] - 3s 56us/sample - loss: 0.3190 - rounded_accuracy: 0.9162 - val_loss: 0.3195 - val_rounded_accuracy: 0.9146
Epoch 9/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3180 - rounded_accuracy: 0.9169 - val_loss: 0.3197 - val_rounded_accuracy: 0.9151
Epoch 10/25
55000/55000 [==============================] - 3s 60us/sample - loss: 0.3172 - rounded_accuracy: 0.9178 - val_loss: 0.3169 - val_rounded_accuracy: 0.9192
Epoch 11/25
55000/55000 [==============================] - 3s 57us/sample - loss: 0.3165 - rounded_accuracy: 0.9183 - val_loss: 0.3197 - val_rounded_accuracy: 0.9177
Epoch 12/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3159 - rounded_accuracy: 0.9188 - val_loss: 0.3168 - val_rounded_accuracy: 0.9185
Epoch 13/25
55000/55000 [==============================] - 3s 62us/sample - loss: 0.3154 - rounded_accuracy: 0.9193 - val_loss: 0.3175 - val_rounded_accuracy: 0.9178
Epoch 14/25
55000/55000 [==============================] - 4s 64us/sample - loss: 0.3150 - rounded_accuracy: 0.9197 - val_loss: 0.3170 - val_rounded_accuracy: 0.9201
Epoch 15/25
55000/55000 [==============================] - 3s 60us/sample - loss: 0.3145 - rounded_accuracy: 0.9199 - val_loss: 0.3177 - val_rounded_accuracy: 0.9202
Epoch 16/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3141 - rounded_accuracy: 0.9202 - val_loss: 0.3161 - val_rounded_accuracy: 0.9206
Epoch 17/25
55000/55000 [==============================] - 3s 61us/sample - loss: 0.3138 - rounded_accuracy: 0.9206 - val_loss: 0.3164 - val_rounded_accuracy: 0.9173
Epoch 18/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3135 - rounded_accuracy: 0.9209 - val_loss: 0.3160 - val_rounded_accuracy: 0.9174
Epoch 19/25
55000/55000 [==============================] - 3s 58us/sample - loss: 0.3132 - rounded_accuracy: 0.9211 - val_loss: 0.3160 - val_rounded_accuracy: 0.9216
Epoch 20/25
55000/55000 [==============================] - 3s 61us/sample - loss: 0.3129 - rounded_accuracy: 0.9213 - val_loss: 0.3155 - val_rounded_accuracy: 0.9212
Epoch 21/25
55000/55000 [==============================] - 3s 61us/sample - loss: 0.3127 - rounded_accuracy: 0.9215 - val_loss: 0.3163 - val_rounded_accuracy: 0.9174
Epoch 22/25
55000/55000 [==============================] - 3s 60us/sample - loss: 0.3125 - rounded_accuracy: 0.9217 - val_loss: 0.3145 - val_rounded_accuracy: 0.9215
Epoch 23/25
55000/55000 [==============================] - 3s 53us/sample - loss: 0.3122 - rounded_accuracy: 0.9219 - val_loss: 0.3158 - val_rounded_accuracy: 0.9201
Epoch 24/25
55000/55000 [==============================] - 3s 56us/sample - loss: 0.3121 - rounded_accuracy: 0.9222 - val_loss: 0.3136 - val_rounded_accuracy: 0.9211
Epoch 25/25
55000/55000 [==============================] - 3s 54us/sample - loss: 0.3118 - rounded_accuracy: 0.9223 - val_loss: 0.3133 - val_rounded_accuracy: 0.9228
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(variational_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_80_0.png" src="../../_images/17_autoencoders_and_gans_80_0.png" />
</div>
</div>
<section id="generate-fashion-images">
<h2>Generate Fashion Images<a class="headerlink" href="#generate-fashion-images" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def plot_multiple_images(images, n_cols=None):
    n_cols = n_cols or len(images)
    n_rows = (len(images) - 1) // n_cols + 1
    if images.shape[-1] == 1:
        images = np.squeeze(images, axis=-1)
    plt.figure(figsize=(n_cols, n_rows))
    for index, image in enumerate(images):
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(image, cmap=&quot;binary&quot;)
        plt.axis(&quot;off&quot;)
</pre></div>
</div>
</div>
</div>
<p>Let’s generate a few random codings, decode them and plot the resulting images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)

codings = tf.random.normal(shape=[12, codings_size])
images = variational_decoder(codings).numpy()
plot_multiple_images(images, 4)
save_fig(&quot;vae_generated_images_plot&quot;, tight_layout=False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure vae_generated_images_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_84_1.png" src="../../_images/17_autoencoders_and_gans_84_1.png" />
</div>
</div>
<p>Now let’s perform semantic interpolation between these images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

codings_grid = tf.reshape(codings, [1, 3, 4, codings_size])
larger_grid = tf.image.resize(codings_grid, size=[5, 7])
interpolated_codings = tf.reshape(larger_grid, [-1, codings_size])
images = variational_decoder(interpolated_codings).numpy()

plt.figure(figsize=(7, 5))
for index, image in enumerate(images):
    plt.subplot(5, 7, index + 1)
    if index%7%2==0 and index//7%2==0:
        plt.gca().get_xaxis().set_visible(False)
        plt.gca().get_yaxis().set_visible(False)
    else:
        plt.axis(&quot;off&quot;)
    plt.imshow(image, cmap=&quot;binary&quot;)
save_fig(&quot;semantic_interpolation_plot&quot;, tight_layout=False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure semantic_interpolation_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_86_1.png" src="../../_images/17_autoencoders_and_gans_86_1.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="generative-adversarial-networks">
<h1>Generative Adversarial Networks<a class="headerlink" href="#generative-adversarial-networks" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(42)
tf.random.set_seed(42)

codings_size = 30

generator = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[codings_size]),
    keras.layers.Dense(150, activation=&quot;selu&quot;),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
discriminator = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(150, activation=&quot;selu&quot;),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
])
gan = keras.models.Sequential([generator, discriminator])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>discriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;)
discriminator.trainable = False
gan.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):
    generator, discriminator = gan.layers
    for epoch in range(n_epochs):
        print(&quot;Epoch {}/{}&quot;.format(epoch + 1, n_epochs))              # not shown in the book
        for X_batch in dataset:
            # phase 1 - training the discriminator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            generated_images = generator(noise)
            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)
            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)
            discriminator.trainable = True
            discriminator.train_on_batch(X_fake_and_real, y1)
            # phase 2 - training the generator
            noise = tf.random.normal(shape=[batch_size, codings_size])
            y2 = tf.constant([[1.]] * batch_size)
            discriminator.trainable = False
            gan.train_on_batch(noise, y2)
        plot_multiple_images(generated_images, 8)                     # not shown
        plt.show()                                                    # not shown
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_gan(gan, dataset, batch_size, codings_size, n_epochs=1)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/1
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_92_1.png" src="../../_images/17_autoencoders_and_gans_92_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)
save_fig(&quot;gan_generated_images_plot&quot;, tight_layout=False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure gan_generated_images_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_93_1.png" src="../../_images/17_autoencoders_and_gans_93_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_gan(gan, dataset, batch_size, codings_size)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_1.png" src="../../_images/17_autoencoders_and_gans_94_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 2/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_3.png" src="../../_images/17_autoencoders_and_gans_94_3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 3/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_5.png" src="../../_images/17_autoencoders_and_gans_94_5.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 4/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_7.png" src="../../_images/17_autoencoders_and_gans_94_7.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 5/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_9.png" src="../../_images/17_autoencoders_and_gans_94_9.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 6/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_11.png" src="../../_images/17_autoencoders_and_gans_94_11.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_13.png" src="../../_images/17_autoencoders_and_gans_94_13.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_15.png" src="../../_images/17_autoencoders_and_gans_94_15.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 9/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_17.png" src="../../_images/17_autoencoders_and_gans_94_17.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 10/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_19.png" src="../../_images/17_autoencoders_and_gans_94_19.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 11/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_21.png" src="../../_images/17_autoencoders_and_gans_94_21.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 12/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_23.png" src="../../_images/17_autoencoders_and_gans_94_23.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 13/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_25.png" src="../../_images/17_autoencoders_and_gans_94_25.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 14/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_27.png" src="../../_images/17_autoencoders_and_gans_94_27.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 15/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_29.png" src="../../_images/17_autoencoders_and_gans_94_29.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 16/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_31.png" src="../../_images/17_autoencoders_and_gans_94_31.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 17/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_33.png" src="../../_images/17_autoencoders_and_gans_94_33.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 18/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_35.png" src="../../_images/17_autoencoders_and_gans_94_35.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 19/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_37.png" src="../../_images/17_autoencoders_and_gans_94_37.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 20/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_39.png" src="../../_images/17_autoencoders_and_gans_94_39.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 21/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_41.png" src="../../_images/17_autoencoders_and_gans_94_41.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 22/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_43.png" src="../../_images/17_autoencoders_and_gans_94_43.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 23/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_45.png" src="../../_images/17_autoencoders_and_gans_94_45.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 24/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_47.png" src="../../_images/17_autoencoders_and_gans_94_47.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 25/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_49.png" src="../../_images/17_autoencoders_and_gans_94_49.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 26/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_51.png" src="../../_images/17_autoencoders_and_gans_94_51.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 27/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_53.png" src="../../_images/17_autoencoders_and_gans_94_53.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 28/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_55.png" src="../../_images/17_autoencoders_and_gans_94_55.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 29/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_57.png" src="../../_images/17_autoencoders_and_gans_94_57.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 30/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_59.png" src="../../_images/17_autoencoders_and_gans_94_59.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 31/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_61.png" src="../../_images/17_autoencoders_and_gans_94_61.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 32/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_63.png" src="../../_images/17_autoencoders_and_gans_94_63.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 33/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_65.png" src="../../_images/17_autoencoders_and_gans_94_65.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 34/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_67.png" src="../../_images/17_autoencoders_and_gans_94_67.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 35/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_69.png" src="../../_images/17_autoencoders_and_gans_94_69.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 36/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_71.png" src="../../_images/17_autoencoders_and_gans_94_71.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 37/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_73.png" src="../../_images/17_autoencoders_and_gans_94_73.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 38/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_75.png" src="../../_images/17_autoencoders_and_gans_94_75.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 39/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_77.png" src="../../_images/17_autoencoders_and_gans_94_77.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 40/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_79.png" src="../../_images/17_autoencoders_and_gans_94_79.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 41/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_81.png" src="../../_images/17_autoencoders_and_gans_94_81.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 42/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_83.png" src="../../_images/17_autoencoders_and_gans_94_83.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 43/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_85.png" src="../../_images/17_autoencoders_and_gans_94_85.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 44/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_87.png" src="../../_images/17_autoencoders_and_gans_94_87.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 45/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_89.png" src="../../_images/17_autoencoders_and_gans_94_89.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 46/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_91.png" src="../../_images/17_autoencoders_and_gans_94_91.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 47/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_93.png" src="../../_images/17_autoencoders_and_gans_94_93.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 48/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_95.png" src="../../_images/17_autoencoders_and_gans_94_95.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 49/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_97.png" src="../../_images/17_autoencoders_and_gans_94_97.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 50/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_94_99.png" src="../../_images/17_autoencoders_and_gans_94_99.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="deep-convolutional-gan">
<h1>Deep Convolutional GAN<a class="headerlink" href="#deep-convolutional-gan" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

codings_size = 100

generator = keras.models.Sequential([
    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),
    keras.layers.Reshape([7, 7, 128]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;,
                                 activation=&quot;selu&quot;),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=&quot;SAME&quot;,
                                 activation=&quot;tanh&quot;),
])
discriminator = keras.models.Sequential([
    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;,
                        activation=keras.layers.LeakyReLU(0.2),
                        input_shape=[28, 28, 1]),
    keras.layers.Dropout(0.4),
    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=&quot;SAME&quot;,
                        activation=keras.layers.LeakyReLU(0.2)),
    keras.layers.Dropout(0.4),
    keras.layers.Flatten(),
    keras.layers.Dense(1, activation=&quot;sigmoid&quot;)
])
gan = keras.models.Sequential([generator, discriminator])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>discriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;)
discriminator.trainable = False
gan.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)
dataset = dataset.shuffle(1000)
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_gan(gan, dataset, batch_size, codings_size)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
Saving figure gan_generated_images_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_1.png" src="../../_images/17_autoencoders_and_gans_100_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 2/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_3.png" src="../../_images/17_autoencoders_and_gans_100_3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 3/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_5.png" src="../../_images/17_autoencoders_and_gans_100_5.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 4/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_7.png" src="../../_images/17_autoencoders_and_gans_100_7.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 5/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_9.png" src="../../_images/17_autoencoders_and_gans_100_9.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 6/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_11.png" src="../../_images/17_autoencoders_and_gans_100_11.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_13.png" src="../../_images/17_autoencoders_and_gans_100_13.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_15.png" src="../../_images/17_autoencoders_and_gans_100_15.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 9/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_17.png" src="../../_images/17_autoencoders_and_gans_100_17.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 10/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_19.png" src="../../_images/17_autoencoders_and_gans_100_19.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 11/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_21.png" src="../../_images/17_autoencoders_and_gans_100_21.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 12/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_23.png" src="../../_images/17_autoencoders_and_gans_100_23.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 13/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_25.png" src="../../_images/17_autoencoders_and_gans_100_25.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 14/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_27.png" src="../../_images/17_autoencoders_and_gans_100_27.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 15/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_29.png" src="../../_images/17_autoencoders_and_gans_100_29.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 16/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_31.png" src="../../_images/17_autoencoders_and_gans_100_31.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 17/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_33.png" src="../../_images/17_autoencoders_and_gans_100_33.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 18/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_35.png" src="../../_images/17_autoencoders_and_gans_100_35.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 19/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_37.png" src="../../_images/17_autoencoders_and_gans_100_37.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 20/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_39.png" src="../../_images/17_autoencoders_and_gans_100_39.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 21/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_41.png" src="../../_images/17_autoencoders_and_gans_100_41.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 22/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_43.png" src="../../_images/17_autoencoders_and_gans_100_43.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 23/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_45.png" src="../../_images/17_autoencoders_and_gans_100_45.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 24/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_47.png" src="../../_images/17_autoencoders_and_gans_100_47.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 25/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_49.png" src="../../_images/17_autoencoders_and_gans_100_49.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 26/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_51.png" src="../../_images/17_autoencoders_and_gans_100_51.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 27/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_53.png" src="../../_images/17_autoencoders_and_gans_100_53.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 28/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_55.png" src="../../_images/17_autoencoders_and_gans_100_55.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 29/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_57.png" src="../../_images/17_autoencoders_and_gans_100_57.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 30/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_59.png" src="../../_images/17_autoencoders_and_gans_100_59.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 31/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_61.png" src="../../_images/17_autoencoders_and_gans_100_61.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 32/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_63.png" src="../../_images/17_autoencoders_and_gans_100_63.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 33/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_65.png" src="../../_images/17_autoencoders_and_gans_100_65.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 34/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_67.png" src="../../_images/17_autoencoders_and_gans_100_67.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 35/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_69.png" src="../../_images/17_autoencoders_and_gans_100_69.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 36/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_71.png" src="../../_images/17_autoencoders_and_gans_100_71.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 37/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_73.png" src="../../_images/17_autoencoders_and_gans_100_73.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 38/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_75.png" src="../../_images/17_autoencoders_and_gans_100_75.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 39/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_77.png" src="../../_images/17_autoencoders_and_gans_100_77.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 40/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_79.png" src="../../_images/17_autoencoders_and_gans_100_79.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 41/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_81.png" src="../../_images/17_autoencoders_and_gans_100_81.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 42/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_83.png" src="../../_images/17_autoencoders_and_gans_100_83.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 43/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_85.png" src="../../_images/17_autoencoders_and_gans_100_85.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 44/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_87.png" src="../../_images/17_autoencoders_and_gans_100_87.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 45/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_89.png" src="../../_images/17_autoencoders_and_gans_100_89.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 46/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_91.png" src="../../_images/17_autoencoders_and_gans_100_91.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 47/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_93.png" src="../../_images/17_autoencoders_and_gans_100_93.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 48/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_95.png" src="../../_images/17_autoencoders_and_gans_100_95.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 49/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_97.png" src="../../_images/17_autoencoders_and_gans_100_97.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 50/50
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_100_99.png" src="../../_images/17_autoencoders_and_gans_100_99.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)
save_fig(&quot;dcgan_generated_images_plot&quot;, tight_layout=False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saving figure dcgan_generated_images_plot
</pre></div>
</div>
<img alt="../../_images/17_autoencoders_and_gans_101_1.png" src="../../_images/17_autoencoders_and_gans_101_1.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="extra-material">
<h1>Extra Material<a class="headerlink" href="#extra-material" title="Permalink to this headline">#</a></h1>
<section id="hashing-using-a-binary-autoencoder">
<h2>Hashing Using a Binary Autoencoder<a class="headerlink" href="#hashing-using-a-binary-autoencoder" title="Permalink to this headline">#</a></h2>
<p>Let’s load the Fashion MNIST dataset again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full.astype(np.float32) / 255
X_test = X_test.astype(np.float32) / 255
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]
</pre></div>
</div>
</div>
</div>
<p>Let’s train an autoencoder where the encoder has a 16-neuron output layer, using the sigmoid activation function, and heavy Gaussian noise just before it. During training, the noise layer will encourage the previous layer to learn to output large values, since small values will just be crushed by the noise. In turn, this means that the output layer will output values close to 0 or 1, thanks to the sigmoid activation function. Once we round the output values to 0s and 1s, we get a 16-bit “semantic” hash. If everything works well, images that look alike will have the same hash. This can be very useful for search engines: for example, if we store each image on a server identified by the image’s semantic hash, then all similar images will end up on the same server. Users of the search engine can then provide an image to search for, and the search engine will compute the image’s hash using the encoder, and quickly return all the images on the server identified by that hash.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

hashing_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.GaussianNoise(15.),
    keras.layers.Dense(16, activation=&quot;sigmoid&quot;),
])
hashing_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[16]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])
hashing_ae = keras.models.Sequential([hashing_encoder, hashing_decoder])
hashing_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.Nadam(),
                   metrics=[rounded_accuracy])
history = hashing_ae.fit(X_train, X_train, epochs=10,
                         validation_data=(X_valid, X_valid))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.4462 - rounded_accuracy: 0.7827 - val_loss: 0.3881 - val_rounded_accuracy: 0.8251
Epoch 2/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3712 - rounded_accuracy: 0.8455 - val_loss: 0.3706 - val_rounded_accuracy: 0.8402
Epoch 3/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3587 - rounded_accuracy: 0.8567 - val_loss: 0.3619 - val_rounded_accuracy: 0.8514
Epoch 4/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3532 - rounded_accuracy: 0.8631 - val_loss: 0.3559 - val_rounded_accuracy: 0.8614
Epoch 5/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3486 - rounded_accuracy: 0.8680 - val_loss: 0.3472 - val_rounded_accuracy: 0.8689
Epoch 6/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3467 - rounded_accuracy: 0.8704 - val_loss: 0.3448 - val_rounded_accuracy: 0.8747
Epoch 7/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3435 - rounded_accuracy: 0.8734 - val_loss: 0.3419 - val_rounded_accuracy: 0.8750
Epoch 8/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3411 - rounded_accuracy: 0.8756 - val_loss: 0.3398 - val_rounded_accuracy: 0.8821
Epoch 9/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3408 - rounded_accuracy: 0.8761 - val_loss: 0.3360 - val_rounded_accuracy: 0.8864
Epoch 10/10
1719/1719 [==============================] - 3s 2ms/step - loss: 0.3386 - rounded_accuracy: 0.8783 - val_loss: 0.3382 - val_rounded_accuracy: 0.8809
</pre></div>
</div>
</div>
</div>
<p>The autoencoder compresses the information so much (down to 16 bits!) that it’s quite lossy, but that’s okay, we’re using it to produce semantic hashes, not to perfectly reconstruct the images:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>show_reconstructions(hashing_ae)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_109_0.png" src="../../_images/17_autoencoders_and_gans_109_0.png" />
</div>
</div>
<p>Notice that the outputs are indeed very close to 0 or 1 (left graph):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plot_activations_histogram(hashing_encoder)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_111_0.png" src="../../_images/17_autoencoders_and_gans_111_0.png" />
</div>
</div>
<p>Now let’s see what the hashes look like for the first few images in the validation set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>hashes = np.round(hashing_encoder.predict(X_valid)).astype(np.int32)
hashes *= np.array([[2**bit for bit in range(16)]])
hashes = hashes.sum(axis=1)
for h in hashes[:5]:
    print(&quot;{:016b}&quot;.format(h))
print(&quot;...&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0000100101011011
0000100100110011
0100100100011011
0001100111001010
0001010100110000
...
</pre></div>
</div>
</div>
</div>
<p>Now let’s find the most common image hashes in the validation set, and display a few images for each hash. In the following image, all the images on a given row have the same hash:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from collections import Counter

n_hashes = 10
n_images = 8

top_hashes = Counter(hashes).most_common(n_hashes)

plt.figure(figsize=(n_images, n_hashes))
for hash_index, (image_hash, hash_count) in enumerate(top_hashes):
    indices = (hashes == image_hash)
    for index, image in enumerate(X_valid[indices][:n_images]):
        plt.subplot(n_hashes, n_images, hash_index * n_images + index + 1)
        plt.imshow(image, cmap=&quot;binary&quot;)
        plt.axis(&quot;off&quot;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_115_0.png" src="../../_images/17_autoencoders_and_gans_115_0.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-solutions">
<h1>Exercise Solutions<a class="headerlink" href="#exercise-solutions" title="Permalink to this headline">#</a></h1>
<section id="to-8">
<h2>1. to 8.<a class="headerlink" href="#to-8" title="Permalink to this headline">#</a></h2>
<p>See Appendix A.</p>
</section>
<section id="id1">
<h2>9.<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p><em>Exercise: Try using a denoising autoencoder to pretrain an image classifier. You can use MNIST (the simplest option), or a more complex image dataset such as <a class="reference external" href="https://homl.info/122">CIFAR10</a> if you want a bigger challenge. Regardless of the dataset you’re using, follow these steps:</em></p>
<ul class="simple">
<li><p>Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set.</p></li>
<li><p>Check that the images are fairly well reconstructed. Visualize the images that most activate each neuron in the coding layer.</p></li>
<li><p>Build a classification DNN, reusing the lower layers of the autoencoder. Train it using only 500 images from the training set. Does it perform better with or without pretraining?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>[X_train, y_train], [X_test, y_test] = keras.datasets.cifar10.load_data()
X_train = X_train / 255
X_test = X_test / 255
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tf.random.set_seed(42)
np.random.seed(42)

denoising_encoder = keras.models.Sequential([
    keras.layers.GaussianNoise(0.1, input_shape=[32, 32, 3]),
    keras.layers.Conv2D(32, kernel_size=3, padding=&quot;same&quot;, activation=&quot;relu&quot;),
    keras.layers.MaxPool2D(),
    keras.layers.Flatten(),
    keras.layers.Dense(512, activation=&quot;relu&quot;),
])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>denoising_encoder.summary()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_105&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gaussian_noise_36 (GaussianN (None, 32, 32, 3)         0         
_________________________________________________________________
conv2d_51 (Conv2D)           (None, 32, 32, 32)        896       
_________________________________________________________________
max_pooling2d_56 (MaxPooling (None, 16, 16, 32)        0         
_________________________________________________________________
flatten_20 (Flatten)         (None, 8192)              0         
_________________________________________________________________
dense_70 (Dense)             (None, 512)               4194816   
=================================================================
Total params: 4,195,712
Trainable params: 4,195,712
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>denoising_decoder = keras.models.Sequential([
    keras.layers.Dense(16 * 16 * 32, activation=&quot;relu&quot;, input_shape=[512]),
    keras.layers.Reshape([16, 16, 32]),
    keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2,
                                 padding=&quot;same&quot;, activation=&quot;sigmoid&quot;)
])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>denoising_decoder.summary()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_106&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_71 (Dense)             (None, 8192)              4202496   
_________________________________________________________________
reshape_22 (Reshape)         (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_transpose_62 (Conv2DT (None, 32, 32, 3)         867       
=================================================================
Total params: 4,203,363
Trainable params: 4,203,363
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>denoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])
denoising_ae.compile(loss=&quot;binary_crossentropy&quot;, optimizer=keras.optimizers.Nadam(),
                     metrics=[&quot;mse&quot;])
history = denoising_ae.fit(X_train, X_train, epochs=10,
                           validation_data=(X_test, X_test))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 50000 samples, validate on 10000 samples
Epoch 1/10
50000/50000 [==============================] - 160s 3ms/sample - loss: 0.5936 - mse: 0.0187 - val_loss: 0.5849 - val_mse: 0.0143
Epoch 2/10
50000/50000 [==============================] - 169s 3ms/sample - loss: 0.5727 - mse: 0.0100 - val_loss: 0.5783 - val_mse: 0.0117
Epoch 3/10
50000/50000 [==============================] - 183s 4ms/sample - loss: 0.5676 - mse: 0.0080 - val_loss: 0.5715 - val_mse: 0.0090
Epoch 4/10
50000/50000 [==============================] - 182s 4ms/sample - loss: 0.5653 - mse: 0.0071 - val_loss: 0.5695 - val_mse: 0.0083
Epoch 5/10
50000/50000 [==============================] - 185s 4ms/sample - loss: 0.5639 - mse: 0.0066 - val_loss: 0.5687 - val_mse: 0.0079
Epoch 6/10
50000/50000 [==============================] - 158s 3ms/sample - loss: 0.5629 - mse: 0.0062 - val_loss: 0.5669 - val_mse: 0.0072
Epoch 7/10
50000/50000 [==============================] - 157s 3ms/sample - loss: 0.5622 - mse: 0.0060 - val_loss: 0.5653 - val_mse: 0.0066
Epoch 8/10
50000/50000 [==============================] - 157s 3ms/sample - loss: 0.5618 - mse: 0.0058 - val_loss: 0.5651 - val_mse: 0.0065
Epoch 9/10
50000/50000 [==============================] - 159s 3ms/sample - loss: 0.5615 - mse: 0.0057 - val_loss: 0.5650 - val_mse: 0.0066
Epoch 10/10
50000/50000 [==============================] - 160s 3ms/sample - loss: 0.5612 - mse: 0.0056 - val_loss: 0.5637 - val_mse: 0.0060
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n_images = 5
new_images = X_test[:n_images]
new_images_noisy = new_images + np.random.randn(n_images, 32, 32, 3) * 0.1
new_images_denoised = denoising_ae.predict(new_images_noisy)

plt.figure(figsize=(6, n_images * 2))
for index in range(n_images):
    plt.subplot(n_images, 3, index * 3 + 1)
    plt.imshow(new_images[index])
    plt.axis(&#39;off&#39;)
    if index == 0:
        plt.title(&quot;Original&quot;)
    plt.subplot(n_images, 3, index * 3 + 2)
    plt.imshow(np.clip(new_images_noisy[index], 0., 1.))
    plt.axis(&#39;off&#39;)
    if index == 0:
        plt.title(&quot;Noisy&quot;)
    plt.subplot(n_images, 3, index * 3 + 3)
    plt.imshow(new_images_denoised[index])
    plt.axis(&#39;off&#39;)
    if index == 0:
        plt.title(&quot;Denoised&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/17_autoencoders_and_gans_125_0.png" src="../../_images/17_autoencoders_and_gans_125_0.png" />
</div>
</div>
</section>
<section id="id2">
<h2>10.<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p><em>Exercise: Train a variational autoencoder on the image dataset of your choice, and use it to generate images. Alternatively, you can try to find an unlabeled dataset that you are interested in and see if you can generate new samples.</em></p>
</section>
<section id="id3">
<h2>11.<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p><em>Exercise: Train a DCGAN to tackle the image dataset of your choice, and use it to generate images. Add experience replay and see if this helps. Turn it into a conditional GAN where you can control the generated class.</em></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./handson-ml2/original"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="16_nlp_with_rnns_and_attention.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Chapter 16 – Natural Language Processing with RNNs and Attention**</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="18_reinforcement_learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 18 – Reinforcement Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Daniel Kapitan<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>